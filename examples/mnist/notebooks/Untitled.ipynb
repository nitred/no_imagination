{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib.layers import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class BaseOps(object):\n",
    "    \"\"\".\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\".\"\"\"\n",
    "        pass\n",
    "\n",
    "    def __weight_variable(self, shape, initializer=tf.truncated_normal_initializer(stddev=0.1)):\n",
    "        \"\"\".\"\"\"\n",
    "        return tf.get_variable(\"weights\", shape=shape, dtype=tf.float32, initializer=initializer)\n",
    "\n",
    "    def __bias_variable(self, shape, initializer=tf.constant_initializer(value=0.0)):\n",
    "        \"\"\".\"\"\"\n",
    "        return tf.get_variable(\"biases\", shape=shape, dtype=tf.float32, initializer=initializer)\n",
    "\n",
    "    def __leaky_relu(self, inputs, alpha=0.2):\n",
    "        \"\"\".\"\"\"\n",
    "        return tf.maximum(alpha * inputs, inputs, \"leaky_relu\")\n",
    "\n",
    "    def __batch_norm(self, x):\n",
    "        return tf.contrib.layers.batch_norm(x, decay=0.9, scale=True, updates_collections=None)\n",
    "\n",
    "    def fc(self, inputs, output_dim, activation, keep_prob=None):\n",
    "        \"\"\".\"\"\"\n",
    "        with tf.variable_scope(\"fc\"):\n",
    "            input_dim = inputs.shape[-1]\n",
    "            w_fc = self.__weight_variable(shape=[input_dim, output_dim])\n",
    "            b_fc = self.__bias_variable(shape=[output_dim])\n",
    "            h_fc_logit = tf.matmul(inputs, w_fc) + b_fc\n",
    "\n",
    "            if activation == \"linear\":\n",
    "                h_fc = h_fc_logit\n",
    "            elif activation == \"leaky_relu\":\n",
    "                h_fc = self.__leaky_relu(h_fc_logit, alpha=0.2)\n",
    "            else:\n",
    "                h_fc = activation(h_fc_logit)\n",
    "\n",
    "            if keep_prob is not None:\n",
    "                h_fc = tf.nn.dropout(h_fc, keep_prob=keep_prob)\n",
    "\n",
    "            return h_fc\n",
    "        \n",
    "    def __max_unpool(pool, ind, ksize=[1, 2, 2, 1], scope='unpool'):\n",
    "        \"\"\".\n",
    "        \n",
    "           Unpooling layer after max_pool_with_argmax.\n",
    "           Args:\n",
    "               pool:   max pooled output tensor\n",
    "               ind:      argmax indices\n",
    "               ksize:     ksize is the same as for the pool\n",
    "           Return:\n",
    "               unpool:    unpooling tensor\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(scope):\n",
    "            input_shape =  tf.shape(pool)\n",
    "            output_shape = [input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3]]\n",
    "\n",
    "            flat_input_size = tf.cumprod(input_shape)[-1]\n",
    "            flat_output_shape = tf.stack([output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]])\n",
    "\n",
    "            pool_ = tf.reshape(pool, tf.stack([flat_input_size]))\n",
    "            batch_range = tf.reshape(tf.range(tf.cast(output_shape[0], tf.int64), dtype=ind.dtype), \n",
    "                                              shape=tf.stack([input_shape[0], 1, 1, 1]))\n",
    "            b = tf.ones_like(ind) * batch_range\n",
    "            b = tf.reshape(b, tf.stack([flat_input_size, 1]))\n",
    "            ind_ = tf.reshape(ind, tf.stack([flat_input_size, 1]))\n",
    "            ind_ = tf.concat([b, ind_], 1)\n",
    "\n",
    "            ret = tf.scatter_nd(ind_, pool_, shape=tf.cast(flat_output_shape, tf.int64))\n",
    "            ret = tf.reshape(ret, tf.stack(output_shape))\n",
    "            return ret\n",
    "\n",
    "    def conv(self, inputs, filter_shape, activation, stride=[1, 1, 1, 1], pool=False, pool_stride=[1, 2, 2, 1]):\n",
    "        \"\"\".\n",
    "\n",
    "        Filter Arguments Example:\n",
    "        >>> filter_shape = [5, 5, 1, 32]  # 5x5 filter and 1 channel, 32 filters or feature maps\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"conv\"):\n",
    "            W_conv = self.__weight_variable(filter_shape)\n",
    "            b_conv = self.__bias_variable([filter_shape[-1]])\n",
    "            output = tf.nn.conv2d(input=inputs, filter=W_conv, strides=stride, padding=\"SAME\")\n",
    "            output = activation(output + b_conv)\n",
    "\n",
    "            if pool:\n",
    "                output = tf.nn.max_pool(output, ksize=pool_stride, strides=pool_stride, padding=\"SAME\")\n",
    "\n",
    "            return output\n",
    "\n",
    "    def deconv(self, inputs, filter_shape, output_shape, activation, stride=[1, 1, 1, 1], unpool=False, unpool_stride=[1, 2, 2, 1]):\n",
    "        \"\"\".\n",
    "        \n",
    "        Tip: Output shape of conv should be the input shape of deconv.\n",
    "        \n",
    "        Filter Arguments Example:\n",
    "        >>> conv_input_shape = [-1, 28, 28, 1]\n",
    "        >>> conv_filter_shape = [5, 5, 1, 32]\n",
    "        >>> conv_output_shape = [-1, 28, 28, 32]\n",
    "        \n",
    "        >>> deconv_input_shape = [-1, 28, 28, 32]\n",
    "        >>> deconv_filter_shape = [5, 5, 1, 1]\n",
    "        >>> deconv_output_shape = [-1, 28, 28, 1]\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"deconv\"):\n",
    "            if unpool:\n",
    "                inputs = self.__max_unpool(inputs, ksize=unpool_stride, strides=unpool_stride, padding=\"SAME\")\n",
    "                \n",
    "            W_deconv = self.__weight_variable(filter_shape)\n",
    "            b_deconv = self.__bias_variable([filter_shape[-1]])\n",
    "            output = tf.nn.conv2d_transpose(inputs, filter=W_deconv, output_shape=output_shape, strides=stride, padding=\"SAME\")\n",
    "            output = activation(output + b_deconv)\n",
    "            \n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(BaseOps):\n",
    "    \"\"\".\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\".\"\"\"\n",
    "        mnist_data_dir = \"/home/nitred/.no_imagination/mnist/dataset/\"\n",
    "        self.mnist_data = input_data.read_data_sets(mnist_data_dir, one_hot=True)\n",
    "        self.__build_model()\n",
    "#         self.__build_accuracy_computation()\n",
    "#         self.__start_session()\n",
    "\n",
    "    def __build_model(self):\n",
    "        \"\"\".\"\"\"\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            with tf.variable_scope(\"inputs\"):\n",
    "                self.x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "                self.y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "                self.keep_prob = tf.placeholder(tf.float32)\n",
    "                x_image = tf.reshape(self.x, [-1, 28, 28, 1])\n",
    "\n",
    "            with tf.variable_scope(\"conv1\"):\n",
    "                conv1 = self.conv(inputs=x_image, filter_shape=[5, 5, 1, 32], activation=tf.nn.relu, stride=[1, 1, 1, 1],\n",
    "                                  pool=True, pool_stride=[1, 2, 2, 1])\n",
    "                \n",
    "            print(conv1.shape)\n",
    "            print(tf.reshape(conv1, [-1, 14*14*32]).shape)\n",
    "            print(flatten(conv1).shape)\n",
    "            # -1, 14, 14, 32\n",
    "            \n",
    "            with tf.variable_scope(\"deconv1\"):\n",
    "                deconv1 = self.deconv(inputs=conv1, filter_shape=[5, 5, 32, 1], output_shape=[10, 14, 14, 1], activation=tf.nn.relu, stride=[1, 1, 1, 1],\n",
    "                                      unpool=False, unpool_stride=[1, 2, 2, 1])\n",
    "                \n",
    "                \n",
    "            print(deconv1.shape)\n",
    "            \n",
    "#             tf.nn.conv2d_transpose(conv1, filter=[])\n",
    "\n",
    "#             with tf.variable_scope(\"conv2\"):\n",
    "#                 conv2 = self.conv(inputs=conv1, filter=[5, 5, 32], n_filters=64, activation=tf.nn.relu, stride=[1, 1, 1, 1],\n",
    "#                                   pool=True, pool_stride=[1, 2, 2, 1])\n",
    "\n",
    "#             with tf.variable_scope(\"fc1\"):\n",
    "#                 h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "#                 fc1 = self.fc(inputs=conv2, input_dim=, output_dim=, activation=, keep_prob=None)\n",
    "\n",
    "#                 h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "#                 h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n",
    "\n",
    "#             with tf.variable_scope(\"fc2\"):\n",
    "#                 W_fc2 = self.__weight_variable([1024, 10])\n",
    "#                 b_fc2 = self.__bias_variable([10])\n",
    "#                 self.y_out = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "#             cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.y_out, labels=self.y))\n",
    "#             optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "#             self.train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "#     def __build_accuracy_computation(self):\n",
    "#         \"\"\".\"\"\"\n",
    "#         with self.g.as_default():\n",
    "#             # boolean prediction\n",
    "#             correct_prediction = tf.equal(tf.argmax(self.y_out, 1), tf.argmax(self.y, 1))\n",
    "#             self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#     def __start_session(self):\n",
    "#         \"\"\".\"\"\"\n",
    "#         with self.g.as_default():\n",
    "#             self.sess = tf.Session(graph=self.g)\n",
    "#             self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#     def run(self, epochs=20000, batch_size=50, keep_prob=0.5, summary_epochs=500):\n",
    "#         \"\"\".\"\"\"\n",
    "#         for i in range(epochs):\n",
    "#             batch_x, batch_y = self.mnist_data.train.next_batch(batch_size)\n",
    "#             self.sess.run(self.train_step, feed_dict={self.x: batch_x, self.y: batch_y, self.keep_prob: keep_prob})\n",
    "#             if i % summary_epochs == 0:\n",
    "#                 print(self.sess.run(self.accuracy, feed_dict={self.x: self.mnist_data.test.images,\n",
    "#                                                               self.y: self.mnist_data.test.labels,\n",
    "#                                                               self.keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/nitred/.no_imagination/mnist/dataset/train-images-idx3-ubyte.gz\n",
      "Extracting /home/nitred/.no_imagination/mnist/dataset/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/nitred/.no_imagination/mnist/dataset/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/nitred/.no_imagination/mnist/dataset/t10k-labels-idx1-ubyte.gz\n",
      "(?, 14, 14, 32)\n",
      "(?, 6272)\n",
      "(?, 6272)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "input channels does not match filter's input channels, 32 != 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7f919448f621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmnist_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# mnist_cnn.__build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# mnist_cnn.run(epochs=20000, batch_size=50, summary_epochs=500)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-01ba545505c6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmnist_data_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/nitred/.no_imagination/mnist/dataset/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#         self.__build_accuracy_computation()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#         self.__start_session()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-01ba545505c6>\u001b[0m in \u001b[0;36m__build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"deconv1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 deconv1 = self.deconv(inputs=conv1, filter_shape=[5, 5, 32, 1], output_shape=[10, 14, 14, 1], activation=tf.nn.relu, stride=[1, 1, 1, 1],\n\u001b[0;32m---> 33\u001b[0;31m                                       unpool=False, unpool_stride=[1, 2, 2, 1])\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-1c09b2ab684b>\u001b[0m in \u001b[0;36mdeconv\u001b[0;34m(self, inputs, filter_shape, output_shape, activation, stride, unpool, unpool_stride)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mW_deconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__weight_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mb_deconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__bias_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilter_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_transpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mW_deconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_deconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/no_imagination/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconv2d_transpose\u001b[0;34m(value, filter, output_shape, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   1078\u001b[0m       raise ValueError(\"input channels does not match filter's input channels, \"\n\u001b[1;32m   1079\u001b[0m                        \"{} != {}\".format(value.get_shape()[3], filter.get_shape(\n\u001b[0;32m-> 1080\u001b[0;31m                        )[3]))\n\u001b[0m\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0moutput_shape_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"output_shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: input channels does not match filter's input channels, 32 != 1"
     ]
    }
   ],
   "source": [
    "mnist_cnn = CNN()\n",
    "# mnist_cnn.__build\n",
    "# mnist_cnn.run(epochs=20000, batch_size=50, summary_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
